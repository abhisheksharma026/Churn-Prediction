{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('ClassificationProblem1.txt',sep=\" \",header='infer',delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F14</th>\n",
       "      <th>F15</th>\n",
       "      <th>F16</th>\n",
       "      <th>F17</th>\n",
       "      <th>F18</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.500340</td>\n",
       "      <td>0.489860</td>\n",
       "      <td>0.902413</td>\n",
       "      <td>7934</td>\n",
       "      <td>-6970</td>\n",
       "      <td>-5714</td>\n",
       "      <td>9982</td>\n",
       "      <td>-5697</td>\n",
       "      <td>...</td>\n",
       "      <td>-3433637453</td>\n",
       "      <td>10/4/1986</td>\n",
       "      <td>9/6/1992</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>706</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.321128</td>\n",
       "      <td>0.281119</td>\n",
       "      <td>0.907283</td>\n",
       "      <td>0.772159</td>\n",
       "      <td>-8238</td>\n",
       "      <td>1219</td>\n",
       "      <td>1663</td>\n",
       "      <td>1287</td>\n",
       "      <td>-3658</td>\n",
       "      <td>...</td>\n",
       "      <td>609277486</td>\n",
       "      <td>2/24/1979</td>\n",
       "      <td>1/5/1983</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>423</td>\n",
       "      <td>206</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.893441</td>\n",
       "      <td>0.622005</td>\n",
       "      <td>0.998776</td>\n",
       "      <td>0.098386</td>\n",
       "      <td>8540</td>\n",
       "      <td>5266</td>\n",
       "      <td>-9377</td>\n",
       "      <td>-3504</td>\n",
       "      <td>-4511</td>\n",
       "      <td>...</td>\n",
       "      <td>-8977995005</td>\n",
       "      <td>1/12/1989</td>\n",
       "      <td>11/22/1986</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>703</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.320641</td>\n",
       "      <td>0.957234</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.646479</td>\n",
       "      <td>-7772</td>\n",
       "      <td>-383</td>\n",
       "      <td>9681</td>\n",
       "      <td>-8661</td>\n",
       "      <td>3474</td>\n",
       "      <td>...</td>\n",
       "      <td>4868760308</td>\n",
       "      <td>2/18/1982</td>\n",
       "      <td>6/10/1992</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>122</td>\n",
       "      <td>304</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.475961</td>\n",
       "      <td>0.623008</td>\n",
       "      <td>0.544988</td>\n",
       "      <td>0.159709</td>\n",
       "      <td>1571</td>\n",
       "      <td>-8039</td>\n",
       "      <td>-7961</td>\n",
       "      <td>-2385</td>\n",
       "      <td>4407</td>\n",
       "      <td>...</td>\n",
       "      <td>9757408267</td>\n",
       "      <td>4/10/1987</td>\n",
       "      <td>10/19/1985</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>486</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index        F1        F2        F3        F4    F5    F6    F7    F8  \\\n",
       "0      1  0.224506  0.500340  0.489860  0.902413  7934 -6970 -5714  9982   \n",
       "1      2  0.321128  0.281119  0.907283  0.772159 -8238  1219  1663  1287   \n",
       "2      3  0.893441  0.622005  0.998776  0.098386  8540  5266 -9377 -3504   \n",
       "3      4  0.320641  0.957234  0.346000  0.646479 -7772  -383  9681 -8661   \n",
       "4      5  0.475961  0.623008  0.544988  0.159709  1571 -8039 -7961 -2385   \n",
       "\n",
       "     F9 ...         F14        F15         F16  F17  F18  F19  F20  F21  F22  \\\n",
       "0 -5697 ... -3433637453  10/4/1986    9/6/1992    2    1  706  305    1    2   \n",
       "1 -3658 ...   609277486  2/24/1979    1/5/1983    1    1  423  206   18    7   \n",
       "2 -4511 ... -8977995005  1/12/1989  11/22/1986    2    1  703  315    1    4   \n",
       "3  3474 ...  4868760308  2/18/1982   6/10/1992    1    1  122  304   15    1   \n",
       "4  4407 ...  9757408267  4/10/1987  10/19/1985    1    1  486  240    1    1   \n",
       "\n",
       "   C  \n",
       "0  0  \n",
       "1  1  \n",
       "2  0  \n",
       "3  0  \n",
       "4  0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 101180 entries, 0 to 101179\n",
      "Data columns (total 24 columns):\n",
      "Index    101180 non-null int64\n",
      "F1       101180 non-null float64\n",
      "F2       101180 non-null float64\n",
      "F3       101180 non-null float64\n",
      "F4       101180 non-null float64\n",
      "F5       101180 non-null int64\n",
      "F6       101180 non-null int64\n",
      "F7       101180 non-null int64\n",
      "F8       101180 non-null int64\n",
      "F9       101180 non-null int64\n",
      "F10      101180 non-null int64\n",
      "F11      101180 non-null int64\n",
      "F12      101180 non-null int64\n",
      "F13      101180 non-null int64\n",
      "F14      101180 non-null int64\n",
      "F15      101180 non-null object\n",
      "F16      101180 non-null object\n",
      "F17      101180 non-null int64\n",
      "F18      101180 non-null int64\n",
      "F19      101180 non-null int64\n",
      "F20      101180 non-null int64\n",
      "F21      101180 non-null int64\n",
      "F22      101180 non-null int64\n",
      "C        101180 non-null int64\n",
      "dtypes: float64(4), int64(18), object(2)\n",
      "memory usage: 17.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info() # no null values #F15,F16 object type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    " \n",
    "year = lambda x: datetime.strptime(x,'%m/%d/%Y').year\n",
    "df['yearF15'] = df['F15'].map(year)\n",
    "df['yearF16'] = df['F16'].map(year)\n",
    "\n",
    "month = lambda x: datetime.strptime(x,'%m/%d/%Y').month\n",
    "df['monthF15'] = df['F15'].map(month)\n",
    "df['monthF16'] = df['F16'].map(month)\n",
    "\n",
    "day = lambda x: datetime.strptime(x,'%m/%d/%Y').day\n",
    "df['dayF15'] = df['F15'].map(day)\n",
    "df['dayF16'] = df['F16'].map(day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['F15','F16'],axis=1,inplace=True)\n",
    "C= df.pop('C') # moving to the end\n",
    "df['C']= C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    76353\n",
       "True     24827\n",
       "Name: C, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df['C']==1).value_counts() # imbalanced class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['F1', 'F2', 'F3', 'F4', 'F5', 'F6', 'F7', 'F8', 'F9', 'F10',\n",
    "       'F11', 'F12', 'F13', 'F14', 'F17', 'F18', 'F19', 'F20', 'F21', 'F22',\n",
    "       'yearF15', 'yearF16', 'monthF15', 'monthF16', 'dayF15', 'dayF16']]\n",
    "\n",
    "y = df['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10468 12405]\n",
      " [  783  6698]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.46      0.61     22873\n",
      "          1       0.35      0.90      0.50      7481\n",
      "\n",
      "avg / total       0.79      0.57      0.59     30354\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "import warnings\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(penalty='l2',solver='sag',class_weight='balanced',n_jobs=-1)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\abhishek .desktop-aklkmr5\\python\\python36-32\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 17 (0.056943)\n",
      "2. feature 16 (0.056504)\n",
      "3. feature 3 (0.039171)\n",
      "4. feature 1 (0.038637)\n",
      "5. feature 13 (0.038431)\n",
      "6. feature 12 (0.038334)\n",
      "7. feature 0 (0.038024)\n",
      "8. feature 5 (0.037971)\n",
      "9. feature 9 (0.037922)\n",
      "10. feature 2 (0.037920)\n",
      "11. feature 6 (0.037916)\n",
      "12. feature 8 (0.037870)\n",
      "13. feature 7 (0.037833)\n",
      "14. feature 11 (0.037775)\n",
      "15. feature 10 (0.037746)\n",
      "16. feature 4 (0.037740)\n",
      "17. feature 24 (0.037430)\n",
      "18. feature 25 (0.037430)\n",
      "19. feature 20 (0.036882)\n",
      "20. feature 21 (0.036756)\n",
      "21. feature 14 (0.036166)\n",
      "22. feature 23 (0.036146)\n",
      "23. feature 22 (0.036041)\n",
      "24. feature 15 (0.032829)\n",
      "25. feature 19 (0.031909)\n",
      "26. feature 18 (0.031672)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build a forest and compute the feature importances\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import numpy as np\n",
    "forest = ExtraTreesClassifier(n_estimators=100,\n",
    "                              random_state=0)\n",
    "\n",
    "forest.fit(X, y)\n",
    "importances = forest.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Print the feature ranking\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>yearF15</th>\n",
       "      <th>yearF16</th>\n",
       "      <th>monthF15</th>\n",
       "      <th>monthF16</th>\n",
       "      <th>dayF15</th>\n",
       "      <th>dayF16</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.224506</td>\n",
       "      <td>0.500340</td>\n",
       "      <td>0.489860</td>\n",
       "      <td>0.902413</td>\n",
       "      <td>7934</td>\n",
       "      <td>-6970</td>\n",
       "      <td>-5714</td>\n",
       "      <td>9982</td>\n",
       "      <td>-5697</td>\n",
       "      <td>...</td>\n",
       "      <td>305</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1986</td>\n",
       "      <td>1992</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.321128</td>\n",
       "      <td>0.281119</td>\n",
       "      <td>0.907283</td>\n",
       "      <td>0.772159</td>\n",
       "      <td>-8238</td>\n",
       "      <td>1219</td>\n",
       "      <td>1663</td>\n",
       "      <td>1287</td>\n",
       "      <td>-3658</td>\n",
       "      <td>...</td>\n",
       "      <td>206</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>1979</td>\n",
       "      <td>1983</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.893441</td>\n",
       "      <td>0.622005</td>\n",
       "      <td>0.998776</td>\n",
       "      <td>0.098386</td>\n",
       "      <td>8540</td>\n",
       "      <td>5266</td>\n",
       "      <td>-9377</td>\n",
       "      <td>-3504</td>\n",
       "      <td>-4511</td>\n",
       "      <td>...</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1989</td>\n",
       "      <td>1986</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.320641</td>\n",
       "      <td>0.957234</td>\n",
       "      <td>0.346000</td>\n",
       "      <td>0.646479</td>\n",
       "      <td>-7772</td>\n",
       "      <td>-383</td>\n",
       "      <td>9681</td>\n",
       "      <td>-8661</td>\n",
       "      <td>3474</td>\n",
       "      <td>...</td>\n",
       "      <td>304</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1982</td>\n",
       "      <td>1992</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.475961</td>\n",
       "      <td>0.623008</td>\n",
       "      <td>0.544988</td>\n",
       "      <td>0.159709</td>\n",
       "      <td>1571</td>\n",
       "      <td>-8039</td>\n",
       "      <td>-7961</td>\n",
       "      <td>-2385</td>\n",
       "      <td>4407</td>\n",
       "      <td>...</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>1985</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index        F1        F2        F3        F4    F5    F6    F7    F8  \\\n",
       "0      1  0.224506  0.500340  0.489860  0.902413  7934 -6970 -5714  9982   \n",
       "1      2  0.321128  0.281119  0.907283  0.772159 -8238  1219  1663  1287   \n",
       "2      3  0.893441  0.622005  0.998776  0.098386  8540  5266 -9377 -3504   \n",
       "3      4  0.320641  0.957234  0.346000  0.646479 -7772  -383  9681 -8661   \n",
       "4      5  0.475961  0.623008  0.544988  0.159709  1571 -8039 -7961 -2385   \n",
       "\n",
       "     F9 ...  F20  F21  F22  yearF15  yearF16  monthF15  monthF16  dayF15  \\\n",
       "0 -5697 ...  305    1    2     1986     1992        10         9       4   \n",
       "1 -3658 ...  206   18    7     1979     1983         2         1      24   \n",
       "2 -4511 ...  315    1    4     1989     1986         1        11      12   \n",
       "3  3474 ...  304   15    1     1982     1992         2         6      18   \n",
       "4  4407 ...  240    1    1     1987     1985         4        10      10   \n",
       "\n",
       "   dayF16  C  \n",
       "0       6  0  \n",
       "1       5  1  \n",
       "2      22  0  \n",
       "3      10  0  \n",
       "4      19  0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create correlation matrix to check for columns to drop\n",
    "\n",
    "import numpy as np\n",
    "corr_matrix = df.corr().abs()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "\n",
    "# Drop features \n",
    "df.drop(df.columns[to_drop], axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape Counter({0: 76353, 1: 24827})\n",
      "Resampled dataset shape Counter({0: 76353, 1: 76353})\n"
     ]
    }
   ],
   "source": [
    "# use SMOTE to remove class imbalance \n",
    "\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "print('Original dataset shape {}'.format(Counter(y)))\n",
    "\n",
    "sm = SMOTE()\n",
    "X, y = sm.fit_sample(X, y)\n",
    "\n",
    "print('Resampled dataset shape {}'.format(Counter(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10694 12214]\n",
      " [ 3129 19775]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.77      0.47      0.58     22908\n",
      "          1       0.62      0.86      0.72     22904\n",
      "\n",
      "avg / total       0.70      0.67      0.65     45812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting Logistic Regression to the Training set\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(penalty='l2',solver='sag',class_weight='balanced',n_jobs=-1)\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = lr_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17407  5501]\n",
      " [ 5005 17899]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.76      0.77     22908\n",
      "          1       0.76      0.78      0.77     22904\n",
      "\n",
      "avg / total       0.77      0.77      0.77     45812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_classifier = DecisionTreeClassifier(criterion = 'gini')\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_tree = tree_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred_tree))\n",
    "print(classification_report(y_test,y_pred_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20379  2529]\n",
      " [ 5785 17119]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      0.89      0.83     22908\n",
      "          1       0.87      0.75      0.80     22904\n",
      "\n",
      "avg / total       0.83      0.82      0.82     45812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 50, criterion = 'gini')\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred_rf))\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[21622  1286]\n",
      " [ 6858 16046]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.76      0.94      0.84     22908\n",
      "          1       0.93      0.70      0.80     22904\n",
      "\n",
      "avg / total       0.84      0.82      0.82     45812\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fitting XGBoost to the Training set\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "xgb_classifier = XGBClassifier(max_depth=7,n_estimators=50,n_jobs=-1)\n",
    "xgb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred_xgb = xgb_classifier.predict(X_test)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8241903395433651\n",
      "0.0034376084235530706\n"
     ]
    }
   ],
   "source": [
    "# Applying k-Fold Cross Validation\n",
    "\n",
    "accuracies_xgb = cross_val_score(estimator = xgb_classifier, X = X_train, y = y_train, cv = 10)\n",
    "print(accuracies_xgb.mean())\n",
    "print(accuracies_xgb.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence,Random Forest is the best classifier for this dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
